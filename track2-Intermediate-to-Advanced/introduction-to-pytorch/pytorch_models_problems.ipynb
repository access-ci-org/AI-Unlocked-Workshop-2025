{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd628080-93ba-4633-9474-cd201c9ef48f",
   "metadata": {},
   "source": [
    "# PyTorch Model Notebook #\n",
    "This notebook will allow you to get practice in building and working with PyTorch models.  Code excersises denoted by a problem number (i.e. Problem #1) will include a task and a code block that asks for your solution.  These blocks will be denoted by comments of the form '# YOUR CODE HERE #'.  The code immediately following include assertions that are used to check completeness of the response.  They will raise an exception if the previous solution is not complete or not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7934d40a-5dff-46f5-82ff-750df7005c96",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders\n",
    "\n",
    "Reference:  The Linux Foundation, \"Datasets & DataLoaders - PyTorch Tutorials 2.6.0 +cu124 documentation,\" pytorch.org https://pytorch.org/tutorials/beginner/basics/data_tutorial.html (accessed Mar. 20, 2025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02f8ea-d81e-4cbd-b2a0-9daa4a7d64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c696a-2822-429f-9029-d15ab78880ac",
   "metadata": {},
   "source": [
    "**Problem #1:**  Finish implementing the \"RandDataset\" Dataset by 1) setting \"self.mapping\" to a random tensor of dimension (output_dims, input_dims), 2) implementing the '\\_\\_len\\_\\_' method by returning the length of the dataset, and 3) setting the **output_tensor** = **Mx** in the '\\_\\_getitem\\_\\_' method, where **M** is the \"self.mapping\" tensor and **x** is the \"input_tensor\".  Also remember to implement the \"self.target_transform\" (if not None) on the \"output_tensor\", analagous to the \"self.transform\" already implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12735c50-e079-45b1-90b9-bea833b31f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandDataset(Dataset):\n",
    "    def __init__(self, input_dims, output_dims, length, transform=None, target_transform=None):\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        # YOUR CODE HERE #\n",
    "        self.length = length\n",
    "        \n",
    "    # YOUR CODE HERE #\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_tensor = torch.rand(self.input_dims)\n",
    "        if self.transform:\n",
    "            input_tensor = self.transform(input_tensor)\n",
    "        # YOUR CODE HERE #\n",
    "        return input_tensor, output_tensor\n",
    "\n",
    "assert len(RandDataset(5,10,1000)) == 1000\n",
    "assert (RandDataset(5, 10, 1000)).mapping.shape == (10,5)\n",
    "assert (RandDataset(5, 10, 1000))[1][1].shape[0] == 10\n",
    "assert ((RandDataset(5, 10, 1000, target_transform=lambda x: x + 20))[1][1] > 20).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f8acf-a685-4611-a8d4-033dea5f3b2d",
   "metadata": {},
   "source": [
    "**Problem #2:**  Instatiate the RandDataset class with a length=32000 and variables \"input_dims\" and \"output_dims\".  Set variables named \"input_dims\" and \"output_dims\" to apropriate values and use in the RandDataset instantiation call. Also, instantiate a DataLoader using this dataset object using a \"batch_size\" of 32, already implemented. Name this dataloader object \"rand_dataset\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61505be8-94d1-46d6-afab-1416d1221dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# YOUR CODE HERE #\n",
    "\n",
    "assert input_dims > 0 and output_dims > 0\n",
    "assert len(rand_dataloader.dataset) == 32000\n",
    "assert rand_dataloader.batch_size == 32\n",
    "assert len(rand_dataloader) == 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49cac2-7537-49fb-888f-f5c48a4f3570",
   "metadata": {},
   "source": [
    "**Setting the train and test dataloaders from above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b79a3f-9abd-4993-9a9d-e9002211447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "train_dataloader = rand_dataloader\n",
    "\n",
    "test_dataset = copy.deepcopy(train_dataloader.dataset)\n",
    "test_dataset.length = 1000\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "assert len(test_dataloader.dataset) == 1000\n",
    "assert (train_dataloader.dataset.mapping == test_dataloader.dataset.mapping).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d56cc-e67f-4103-b365-f565ec72595d",
   "metadata": {},
   "source": [
    "## Building the PyTorch Model ##\n",
    "\n",
    "Reference:  The Linux Foundation, \"Build the Neural Network - PyTorch Tutorials 2.6.0 +cu124 documentation,\" pytorch.org https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html (accessed Mar. 13, 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042e401-ef2a-477c-a319-ee5752d49260",
   "metadata": {},
   "source": [
    "**Problem #3:**  Implement a Pytorch model class named \"NNModel\". Fill in the instantiation of the model's layers, which should include a nn.Linear, nn.ReLU, nn.Linear, nn.ReLU, and nn.Linear layers.  There should be **n** input neurons, **h** hidden neurons, and **m** output neurons.  Hint: Both hidden linear layers (first two) should have **h** neurons. Implement the forward computation of the model using the **input_tensor** as input and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05988e93-2a2f-4adb-b15e-6698f8805871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NNModel(nn.Module):\n",
    "    def __init__(self,n,m,h):\n",
    "        super().__init__()\n",
    "\n",
    "        # YOUR CODE HERE #\n",
    "    \n",
    "    def forward(self, input_tensor):\n",
    "        # YOUR CODE HERE #\n",
    "\n",
    "import re\n",
    "assert ((NNModel(input_dims, output_dims, 50))(torch.rand(5,input_dims))).shape == (5,output_dims)\n",
    "assert len(re.findall('Linear', str(NNModel(input_dims, output_dims, 50)))) == 3\n",
    "assert len(re.findall('ReLU()', str(NNModel(input_dims, output_dims, 50)))) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e3e8c-bf0b-447f-8b38-964e83a07702",
   "metadata": {},
   "source": [
    "## Optimizing the PyTorch Model ##\n",
    "\n",
    "Reference:  The Linux Foundation, \"Optimizing Model Parameters - PyTorch Tutorials 2.6.0 +cu124 documentation,\" pytorch.org https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html (accessed Mar. 24, 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6fcb3-e8da-45d5-b425-bd90cbd90da5",
   "metadata": {},
   "source": [
    "**Problem #4:**  Instantiate the nn.MSELoss function with reduction='sum' and name the object, \"my_loss_fn\".  Instantiate the NNModel using \"input_dims\", \"output_dims\", and any number of hidden neurons and name the object, \"my_model\".   Instantiate the optim.SGD optimizer with the model parameters and the learning_rate defined above and name the object, \"my_optimizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d741d4e-2fe3-4683-8383-7afe50861ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "learning_rate = 1e-2\n",
    "epochs = 50\n",
    "tolerance = 1e-2\n",
    "\n",
    "# YOUR CODE HERE #\n",
    "\n",
    "assert isinstance(my_loss_fn, nn.MSELoss)\n",
    "assert my_loss_fn.reduction == 'sum'\n",
    "assert isinstance(my_model, NNModel)\n",
    "assert isinstance(my_optimizer, optim.SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc83551-bdc2-40a5-a5dd-e6cfc1ef1806",
   "metadata": {},
   "source": [
    "**Problem #5:**  Implement the training loop by including the 1) model predictions from the batch inputs, **X**, and calculating the loss via the loss_fn using the predictions and batch outputs, **Y**.  Divide the loss by the number of predictions to get the **avg_loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5fc63-e026-4f6e-90b9-4ae7b4d76910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, device=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,Y) in enumerate(dataloader):\n",
    "        if device:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "        # YOUR CODE HERE #\n",
    "\n",
    "        avg_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (batch+1) %100 == 0:\n",
    "            avg_loss, current = avg_loss.item(), batch * batch_size + len(pred)\n",
    "            print(f\"Avg. loss: {avg_loss:>7f}, [current:{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return True\n",
    "\n",
    "assert train_loop(DataLoader(RandDataset(3,5,batch_size*100),batch_size=batch_size), NNModel(3,5,20), nn.MSELoss(reduction='sum'), optim.SGD((NNModel(3,5,20)).parameters(), lr=learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3ffcf-c6f3-46e1-9f9c-5313599c069c",
   "metadata": {},
   "source": [
    "**Problem #6:**  Implement the test loop by including the 1) model predictions from the batch inputs, **X**, and calculating the test loss via the loss_fn using the predictions and batch outputs, **Y**.  Divide the test_loss by the number of predictions and remember to use the .item() method to extract the scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7448f4-9984-4854-b2c3-c944bed76dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, tolerance, device=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (X,Y) in dataloader:\n",
    "            if device:\n",
    "                X = X.to(device)\n",
    "                Y = Y.to(device)\n",
    "            # YOUR CODE HERE #\n",
    "            correct += ((pred - Y).abs() < tolerance).all(dim=1).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg. loss: {test_loss:>8f}\\n\")\n",
    "    return True\n",
    "\n",
    "assert test_loop(DataLoader(RandDataset(3,5,batch_size),batch_size=batch_size), NNModel(3,5,20), nn.MSELoss(reduction='sum'), tolerance=tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87940ae2-d8f9-4df3-a657-a4c4a2133317",
   "metadata": {},
   "source": [
    "**Implementing the epoch loop and running the training loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502ac28-f671-4fcb-9d48-ec983a5f1b70",
   "metadata": {},
   "source": [
    "**Problem #7:**  Implement the epoch loop by using the train_loop and test_loop functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d8bdc0-b401-47a0-8187-781a647c10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_loop(epochs, train_dataloader, test_dataloader, model, loss_fn, optimizer, tolerance, device=None):\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n------------------------------\")\n",
    "        # YOUR CODE HERE #\n",
    "    print(\"Done\")\n",
    "    return True\n",
    "\n",
    "epochs_cpu = 2\n",
    "assert epoch_loop(epochs_cpu, train_dataloader, test_dataloader, my_model, my_loss_fn, my_optimizer, tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad7c87-bd6a-434f-9a75-dbd011f61bc2",
   "metadata": {},
   "source": [
    "## Utilizing the GPU for Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58694019-31f5-47b2-ae3f-3c6d74056ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(torch.cuda.current_device())\n",
    "\n",
    "print(f\"Using device - {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5127c0c2-64a2-4286-bdcc-0487b9233ccc",
   "metadata": {},
   "source": [
    "**Problem #8:**  If a GPU device is available, move \"my_model\" to the device and save as \"my_model_gpu\".  Reinitialize the SGD optimizer using \"my_model_gpu.parameters()\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbab2c-a39b-42c6-9f25-5e184a556c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # YOUR CODE HERE #\n",
    "\n",
    "    assert epoch_loop(epochs, train_dataloader, test_dataloader, my_model_gpu, my_loss_fn, my_optimizer_gpu, tolerance, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5c29a-ea07-4e0e-b0be-0b55752e491f",
   "metadata": {},
   "source": [
    "## Saving and Loading PyTorch Models ##\n",
    "\n",
    "Reference:  The Linux Foundation, \"Save and Load the Model - PyTorch Tutorials 2.6.0 +cu124 documentation,\" pytorch.org https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html (accessed Mar. 24, 2025).\n",
    "\n",
    "Reference:  The Linux Foundation, \"Saving and Loading Models - PyTorch Tutorials 2.6.0 +cu124 documentation,\" pytorch.org https://pytorch.org/tutorials/beginner/saving_loading_models.html (accessed Mar. 24, 2025)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb50f2a9-eddd-482f-80a2-d46183501492",
   "metadata": {},
   "source": [
    "**Problem #9:**  Finish the implementation of the save_model_checkpoint funciton. Add the elements to the save_dict dictionary corresponding to keys, \"model_state_dict\", \"optimizer_state_dict\", and \"epoch\".  The values for these should be the .state_dict() for the model and optimizer and the epoch. Secondly, add the function to save the save_dict as a file given in file_path. Hint:  Use torch.save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c259de-7f33-422c-b61f-1cab2fed3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_checkpoint(model, optimizer, dataloader, epoch, file_path):\n",
    "    dataloader_mapping = dataloader.dataset.mapping\n",
    "    save_dict = dict(\n",
    "        # YOUR CODE HERE #\n",
    "        dataloader_mapping = dataloader_mapping,\n",
    "    )\n",
    "    # YOUR CODE HERE #\n",
    "    return True\n",
    "\n",
    "from pathlib import Path\n",
    "assert save_model_checkpoint(NNModel(3,5,20), optim.SGD((NNModel(3,5,20)).parameters(), lr=learning_rate), DataLoader(RandDataset(3,5,batch_size),batch_size=batch_size), 20, Path() / \"test_checkpoint.pth\")\n",
    "assert not set(['model_state_dict','optimizer_state_dict','epoch','dataloader_mapping']) - set((torch.load(Path() / \"test_checkpoint.pth\", weights_only=True)).keys())\n",
    "for key, val in (torch.load(Path() / \"test_checkpoint.pth\", weights_only=True)).items():\n",
    "    if 'dict' in key:\n",
    "        assert isinstance(val, dict)\n",
    "    elif key == 'epoch':\n",
    "        assert val == 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f0bc1-ada0-486e-ac51-00296faee9b7",
   "metadata": {},
   "source": [
    "**Problem #9:**  Finish the implementation of the \"restore_model_checkpoint\" function. Load the checkpoint file defined at \"file_path\" using torch.load(...).  Update the \"model\" and \"optimizer\" state_dicts from the checkpoint.  Update the \"epoch\" variable from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b821b-334d-45df-9ff8-860974a93962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model_checkpoint(model, optimizer, train_dataloader, test_dataloader, file_path):\n",
    "    epoch = -1\n",
    "    if file_path.exists():\n",
    "        print(f\"Restarting from checkpoint: {str(file_path)}\")\n",
    "        # YOUR CODE HERE #\n",
    "        train_dataloader.dataset.mapping = checkpoint['dataloader_mapping']\n",
    "        test_dataloader.dataset.mapping = checkpoint['dataloader_mapping']\n",
    "    return epoch\n",
    "\n",
    "test_model = NNModel(3,5,20)\n",
    "test_optim = optim.SGD(test_model.parameters(), lr=learning_rate)\n",
    "assert restore_model_checkpoint(test_model,test_optim,DataLoader(RandDataset(3,5,batch_size),batch_size=batch_size),DataLoader(RandDataset(3,5,batch_size),batch_size=batch_size),Path() / \"test_checkpoint.pth\") == 20\n",
    "(Path() / \"test_checkpoint.pth\").unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b0bc3c-3c57-4151-8aad-9c7d11de06c2",
   "metadata": {},
   "source": [
    "**Problem #10:**  Reimplement the \"epoch_loop\" with the restore_model_checkpoint and save_model_checkpoint functions. The epoch returned by restore_model_checkpoint should be saved to the \"epoch_last\" variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73acc0f1-413d-4d9f-a2a3-abf257b5e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_loop(epochs, train_dataloader, test_dataloader, model, loss_fn, optimizer, tolerance, device=None, file_path=None):\n",
    "    if file_path:\n",
    "        # YOUR CODE HERE #\n",
    "    for t in range(epoch_last+1,epochs):\n",
    "        print(f\"Epoch {t+1}\\n------------------------------\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "        test_loop(test_dataloader, model, loss_fn, tolerance, device)\n",
    "        if file_path:\n",
    "            # YOUR CODE HERE #\n",
    "    print(\"Done\")\n",
    "    return True\n",
    "\n",
    "use_model = NNModel(input_dims, output_dims, 20).to(device)\n",
    "use_optimizer = optim.SGD(use_model.parameters(), lr=learning_rate)\n",
    "my_epochs = 3\n",
    "assert epoch_loop(my_epochs, train_dataloader, test_dataloader, use_model, my_loss_fn, use_optimizer, tolerance, device, Path()/\"my_checkpoint_file.pth\")\n",
    "if torch.cuda.is_available():\n",
    "    my_epochs = epochs\n",
    "else:\n",
    "    my_epochs = 6\n",
    "assert epoch_loop(my_epochs, train_dataloader, test_dataloader, use_model, my_loss_fn, use_optimizer, tolerance, device, Path()/\"my_checkpoint_file.pth\")\n",
    "(Path()/\"my_checkpoint_file.pth\").unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
